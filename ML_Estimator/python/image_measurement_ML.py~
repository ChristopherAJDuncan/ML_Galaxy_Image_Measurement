"""
Author: cajd
Touch Date: 14th May 2015
Purpose: Contains the code to implement a maximum-likelihood based estimator for image model parameters. Contains routines to evaluate the log-Likelihood, and it's derivatives, as well as supplementary definitions such as mimimisation routines (if applicable), noise estimation from an image, and error estiamtion (e.g. Fisher Matrices).

To Do:
(17Sept2015):: Add ability to apply a prior to the likelihood (in which case the method maximises a *Posterior* rather than a likelihood. In this case, lnL -> lnP + lnL and d^n(lnL) -> d^n(lnP) + d^n (lnL)

"""
import numpy as np
import os
from copy import deepcopy

verbose = False
vverbose  = False
debug = False

def estimate_Noise(image, maskCentroid = None):
    """
    Routine which takes in an image and estimates the noise, needed to accurately calculate the expected bias on profile measurements. Where a centroid value is passed, the code uses a form of `curve of growth` to estiamte the noise, by increasing the size of a cricular mask steadily by one pixel around that centroid and looking for convergence (defined here as the point where the difference between loops is minimised), otherwise the full image is used.

    *** Noise is known to be too large when the postage stamps size is not large enough, so that the model makes up a significant percentage of the image. One may therefore expect the noise to be too large for small PS sizes. ***

    Agrees well with GALSIM noise var on all SNR provided masCentroid is accurately placed on source centre (tested for ellipticity = 0.)

    Requires:
    -- image: Image of source (2-dimensional numpy array)
    -- maskCentroid: center of mask - used to iteritively mask out source to get an accurate estimate of the background noise after removing the source. If None, then the noise is returned as the standard deviation of the image without masking applied. If not None, then the noise is minimum of the difference between successive runs where the mask is increased by one pixel each side of the centre as passed in.

    Returns:
    -- result: Scalar giving the standard deviation of the pixel in the input image, after masking (if appropriate).

    NOTE: UNTESTED for multiple images
    
    """


    if(maskCentroid is not None):
        res = np.zeros(max(maskCentroid[0], maskCentroid[1], abs(image.shape[0]-maskCentroid[0]), abs(image.shape[1]-maskCentroid[1])))
    else:
        res = np.zeros(1)
        
    maskRad = 0; con = 0
    tImage = image.copy()
    while True:
        con += 1

        maskRad = (con-1)*1 #Done in pixels

        if(maskCentroid is not None):
            if(len(tImage.shape) == 3):
                for i in range(tImage.shape[0]):
                    tImage[i][maskCentroid[0]-maskRad:maskCentroid[0]+maskRad, maskCentroid[1]-maskRad:maskCentroid[1]+maskRad] = 0.
            else:
                tImage[maskCentroid[0]-maskRad:maskCentroid[0]+maskRad, maskCentroid[1]-maskRad:maskCentroid[1]+maskRad] = 0.

        res[con-1] = tImage.std()

        if(maskCentroid == None):
            break
        elif(con == res.shape[0]):
            break

    if(maskCentroid is not None):
        return res[np.argmin(np.absolute(np.diff(res)))]
    else:
        return res[0]


## -o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o----- Error Estimation -----o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-##

def fisher_Error_ML(ML, fitParams, image, setParams, modelLookup):
    from copy import deepcopy
    """
    Calculates the marginalised fisher error on the set of fitParams (tuple) around maximum-likelihood point ML. As the log-Likelihood depends on the image, the images must be supplied, along with a model dictionary giving the fixed model parameters (setParams), and the modelLookup (this can be None is no lookup is to be used). The model fit is therefore constructed by setParams+{fitParams:ML}.

    Note: As the Fisher Matrix assumes that the likelihood is Gaussian around the ML point (in *parameter* space), this estimate is likely to be inaccurate for parameters which are non-linearly related to the observed image value at any point.

    Uses the fact that for a Gaussian likelihood (on pixel values, not parameters): ddlnP/(dtheta_i dtheta_j) = 1/sigma^2*sum_pix[delI*model_,ij - model_,i*model_,j], where `,i` labels d()/dtheta_i.

    Requires:
    ML: Computed ML point, entered as 1D list/tuple/numpy array
    fitParams: list of strings, labelling the parameters to be fit as defined in model dictionary definition (see default model dictionary definition)
    image: 2D ndarray, containing image postage stamp (image being fit)
    setParams: model dictionary defining all fixed parameters
    modelLookup: modelLookup table as defined in find_ML_Estimator. Can be None if no lookup is used.

    Returns:
    -- err: Tuple containing marginalised Fisher error for all input parameters (in each case all other parameters are considered fixed to ML or input values).

    Tests:
    -- Value of marginalised error is verified to be comparable to the variance over 5x10^5 simulated images for e1, e2 as free parameters without a prior.
    """

    parameters = deepcopy(ML); pLabels = deepcopy(fitParams)

    ddlnL = differentiate_logLikelihood_Gaussian_Analytic(parameters, pLabels, image, setParams, modelLookup = modelLookup, order = 2, signModifier = 1.)
    ddlnL = -1.*ddlnL ##This is now the Fisher Matrix

    Fin = np.linalg.inv(ddlnL)

    return np.sqrt(np.diag(Fin))
    
##-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o---- ML Estimation   ----o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-##
def find_ML_Estimator(image, fitParams, outputHandle = None, setParams = None, modelLookup = None, searchMethod = 'simplex', preSearchMethod = None, Prior = None, bruteRange = None, biasCorrect = 0, calcNoise = None, bcoutputHandle = None, error = 'Fisher', **iParams):
    import scipy.optimize as opt
    import model_Production as modPro
    from surface_Brightness_Profiles import gaussian_SBProfile_Weave
    import measure_Bias as mBias
    from generalManipulation import makeIterableList
    """
    MAIN ROUTINE FOR THIS MODULE. Takes in an image (at minimum) and a set of values which defines the model parameters (fit and those which are free to vary), and returns the parameter values at which the log-Likelihood is minimised (or Likelihood is maximised). Can correct for first order noise bias (if biasCorrect != 0), and an estimate of the error (if error is equal to a set of pre-defined values [see below]).
    
    Requires:
    -- image: 2d array of pixelised image
    -- fitParams: tuple of strings which define the model parameters which are free to vary (those which will be fit). These must satisfy the definition of model parameters as set out in the default model dictionary. If None, then e1, e2 and T are fit (this could be made stricter by removing the default None initialisation, thereby requiring that a set of parameters to be fit is passed in).
    -- outputHandle: handle of the output file. **Result is always appended**. If not passed in, then result is not output. Output is in ASCII form.
    -- setParams: Default model dictionary containing fixed parameters which describes the model being fixed. One part of a two part approach to setting the full model parameter dictionary, along with iParams. If None, then default model dictionary is taken.
    -- modelLookup: Dictionary containing lookup table for pixelised model images, as defined in model_Production module. If None, no lookup is used, and the model is re-evalauted for each change in model parameters.
    -- searchMethod: String detailing which form of minimisation to use. Accepted values are:
    ___ simplex, brent, powell, cg, bfgs, l_bfgs_b, ncg (as defined in SciPy documentation)
    -- preSearchMethod: String detailing initial search over parameter space to find global Minimium, used as an initial guess for refinement with searchMethod. If None, initial guess is set to default passed in by the combination of setParams and iParams. If not None, then code will run an initial, coarse search over the parameter space to attempt to find the global mimima. By default this is switched off. Where preSearchMethod == grid or brute, the a grid based search is used. Where this is used, a range must either be entered by the user through bruteRange, or it is taken from the entered prior information. NOTE: This still uses a typically coarse grid, therefore if the range is too wide then it is possible that the code may still find a local mimimum if this exists within one grid point interval of the global miminum.
    -- Prior: NOT USED YET. Skeleton to allow for a parameter prior structure to be passed in
    -- bruteRange: [nPar, 2] sized tuple setting the range in which the initial preSearchMethod is evaluated, if this is done using a grid or brute method (both equivalent), where nPar is the number of free model parameters being fit. THIS DOES NOT CONSTITUTE A PRIOR, as the refinement may still find an ML value outside this range, however where the global maximum occurs outside this range the returned ML value may be expected to be biased.
    -- biasCorrect: integer, states what level of noise bias to correct the estimate to. Only 1st order correction (biasCorrect == 1) is supported. If biasCorrect == 0, the uncorrected estimate (and error if applicable) are output. If biasCorrect > 0, the uncorrected, corrected and error (if applicable) are output. When used, it is important that *the entered model parameter dictionary contains an accurate measure of the pixel noise of appropriate signal--to--noise, as the analytic bias scales according to both*. Noise can be estimate using estimate_Noise() before entry.
    -- bcOutputhandle: As outputHandle, except for the bias corrected estimator.
    -- error: String detailing error estiamte to output. Supported values are:
    ___ fisher: Marginalised fisher error for each parameter around the ML point. See docstring for fisher_Error_ML().
    ___ brute: UNSUPPORTED, however an error defined on the parameter likelihood itself can be derived if the preSearchMethod and bruteRange is defined such that the Likelihood has *compact support*. If not, then this would be inaccurate (underestimated). Therefore coding for this is deferred until the application of a prior is developed, as use of a prior ensures compact support by default.
    -- iParams: set of optional arguments which, together with setParams, defines the intial model dictionary. Allows parameter values to be input individually on call, and is particularly useful for setting initial guesses where preSearchMethod == None.
    
    
    Model Parameter entry: Model Parameters can be entered using two methods
    ___ setParams: Full Dictionary of initial guess/fixed value for set of parameters. If None, this is set to default set. May not be complete: if not, then model parameters set to default as given in default_ModelParameter_Dictionary()
    ___iParams: generic input which allows model parameters to be set individually. Keys not set are set to default as given by default_ModelParameter_Dictionary(). Where an iParams key is included in the default dictionary, or setParams, it will be updated to this value (**therefore iParams values have preferrence**). If key not present in default is entered, it is ignored
    ___ The initial choice of model parameters (including intial guesses for the minimisation routine where preSearchMethod == False) is thus set as setParams+{iParams}



    Returns:
    Returned: tuple of length equal to fitParams. Gives ML estimator for each fit parameter, with bias corrected version (if biasCorrect != 0) and error (if applicable) aslways in that order.
    """

    ''' Set up defaults '''

    err = None
    
    ##Initialise output tuple
    Returned = []

    ## Exceptions based on input objects
    if(image is None or sum(image.shape) == 0):
        raise RuntimeError('find_ML_Estimator - image supplied is None or uninitialised')
        
    if(len(fitParams) > 2 and modelLookup is not None and modelLookup['useLookup']):
        raise RuntimeError('find_ML_Estimator - Model Lookup is not supported for more than double parameter fits')

    ##Set up initial params, which sets the intial guess or fixed value for the parameters which defines the model
    ##This line sets up the keywords that are accepted by the routine
    ## pixle_Scale and size should be in arsec/pixel and arcsec respectively. If pixel_scale = 1., then size can be interpreted as size in pixels
    ## centroid should be set to the center of the image, here assumed to be the middle pixel

    if(setParams is None):
        initialParams = modPro.default_ModelParameter_Dictionary()
    else:
        initialParams = modPro.default_ModelParameter_Dictionary()
        modPro.update_Dictionary(initialParams, setParams)
        ## Deprecated initialParams.update(setParams)

    modPro.set_modelParameter(initialParams, iParams.keys(), iParams.values())

    ## Define modelParams
    modelParams = deepcopy(initialParams)

    ## Estimate Noise of Image
    if(calcNoise is not None):
        modelParams['noise'] = calcNoise(image, modelParams['centroid'])

    ####### Search lnL for minimum
    #Construct initial guess for free parameters by removing them from dictionary
    x0 = modPro.unpack_Dictionary(modelParams, requested_keys = fitParams)

    ###### Sanity check image dimensions compared to model parameters
    imDim = len(image.shape)
    if(imDim > 2):
        raise ValueError("find_ML_Estimator: Image must not have more than two dimensions. Single postage stamp image must be flattened")
    elif(imDim == 1 and image.shape[0] != np.array(modelParams['stamp_size']).prod()):
        raise ValueError("find_ML_Estimator: Flattened image (1D) length does not correspond to model parameter dimensions")
    elif(imDim == 2 and image.shape[1] != np.array(modelParams['stamp_size']).prod()):
        print 'Image shape: ', image.shape, ' Model shape:' , modelParams['stamp_size']
        raise ValueError("find_ML_Estimator: image sahpe of second dimension is not consistent with expected model parameter dimension. 2D image array must contain multiple images across first dimension, and (flattened) pixels as a data vector in the second dimension: Have you remembered to flatten the image?")


    if(preSearchMethod is not None):
        ## Conduct a presearch of the parameter space to set initial guess (usually grid-based or brute-force)
        if(vverbose or debug):
            print '\n Conducting a pre-search of parameter space to idenitfy global minima'
        if(preSearchMethod.lower() == 'grid' or preSearchMethod.lower() == 'brute'):
            ##Brute force method over a range either set as the prior, or the input range.
            if(bruteRange is not None):
                if(vverbose or debug):
                    print '\n Using user-defined parameter range:', bruteRange

                #x0, fval, bruteGrid, bruteVal
                bruteOut = opt.brute(get_logLikelihood, ranges = bruteRange, args = (fitParams, image, modelParams, modelLookup, 'sum'), finish = None, full_output = True)
                x0, fval, bruteGrid, bruteVal = bruteOut
                ## x0 has len(nParam); fval is scalar; bruteGrid has len(nParam), nGrid*nParam; bruteVal has nGrid*nParam

                ###Evaluate error based on brute by integration - this would only work if bruteRange cover the full range where the PDF is non-zero

                if(error.lower() == 'brute'):
                    raise RuntimeError('find_ML_Estimator - brute labelled as means of evaluating error. This is possbible, but not coded as limitation in use of bruteRange to cover the whole region where the likelihood is non-zero. When a prior is included, this could be taken to be exact, provided one knows the range where the prior has compact support, and the bruteRange reflects this.')
                ## use scipy.integrate.trapz(bruteVal, x = bruteGrid[i], axis = i) with i looping over all parameters (ensure axis set properly...

                

                if(vverbose or debug):
                    print '\n preSearch has found a minimum (on a coarse grid) of:', x0
                
            elif(Prior is not None):
                if(vverbose or debug):
                    print '\n Using prior range'
                raise RuntimeError('find_ML_Estimator - Prior entry has not yet been coded up')

            else:
                raise RuntimeError('find_ML_Estimator - Brute preSearch is active, but prior or range is not set')

    if(debug or vverbose):
        ##Output Model Dictionary and initial guess information
        print 'Model Dictionary:', modelParams
        print '\n Initial Guess:', x0

    ##Find minimum chi^2 using scipy optimize routines
    ##version 11+ maxima = opt.minimize(get_logLikelihood, x0, args = (fitParams, image, modelParams))
    if(searchMethod.lower() == 'simplex'):
        maxima = opt.fmin(get_logLikelihood, x0 = x0, xtol = 0.00001, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    elif(searchMethod.lower() == 'brent'):
        maxima = opt.fmin_brent(get_logLikelihood, x0 = x0, xtol = 0.00001, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    elif(searchMethod.lower() == 'powell'):
        maxima = opt.fmin_powell(get_logLikelihood, x0 = x0, xtol = 0.00001, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    elif(searchMethod.lower() == 'cg'):
        ##Not tested (10Aug)
        maxima = opt.fmin_cg(get_logLikelihood, x0 = x0, fprime = differentiate_logLikelihood_Gaussian_Analytic, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug), ftol = 0.000001)
    elif(searchMethod.lower() == 'bfgs'):
        ##Not tested (10Aug)
        maxima = opt.fmin_bfgs(get_logLikelihood, x0 = x0, fprime = differentiate_logLikelihood_Gaussian_Analytic, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    elif(searchMethod.lower() == 'l_bfgs_b'):
        ##Not tested (10Aug)
        maxima = opt.fmin_l_bfgs_b(get_logLikelihood, x0 = x0, fprime = differentiate_logLikelihood_Gaussian_Analytic, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    elif(searchMethod.lower() == 'ncg'):
        ##Not tested (10Aug)
        maxima = opt.fmin_ncg(get_logLikelihood, x0 = x0, fprime = differentiate_logLikelihood_Gaussian_Analytic, args = (fitParams, image, modelParams, modelLookup, 'sum'), disp = (verbose or debug))
    else:
        raise ValueError('find_ML_Estimator - searchMethod entered is not supported:'+str(searchMethod))

    ##Make numpy array (in the case where 1D is used and scalar is returned):
    if(len(fitParams)==1):
        maxima = np.array(makeIterableList(maxima))
        
    if(vverbose):
        print 'maxima is:', maxima

    if(debug):
        ##Plot and output residual
        print 'Plotting residual..'
        
        fittedParams = deepcopy(modelParams)
        modPro.set_modelParameter(fittedParams, fitParams, maxima)
        ''' Deprecated
        for i in range(len(fitParams)):
            fittedParams[fitParams[i]] =  maxima[i]
        '''
 
        model, disc =  modPro.user_get_Pixelised_Model(fittedParams, sbProfileFunc = gaussian_SBProfile_Weave)
        residual = image
        if(len(image.shape) == 2):
            residual -= image
        elif(len(image.shape) == 3):
            for i in range(image.shape[0]):
                residual[i] -= image[i]
        else:
            raise ValueError("Error calculating residual: Image has an unknown rank")

        import pylab as pl
        ##Plot image and model
        f = pl.figure()
        ax = f.add_subplot(211)
        ax.set_title('Model')
        im = ax.imshow(model, interpolation = 'nearest')
        pl.colorbar(im)
        ax = f.add_subplot(212)
        ax.set_title('Image')
        if(len(image.shape) == 3):
            im = ax.imshow(image[0], interpolation = 'nearest')
        else:
            im = ax.imshow(image, interpolation = 'nearest')
        pl.colorbar(im)

        pl.show()

        ##Plot Residual
        f = pl.figure()
        ax = f.add_subplot(111)
        im = ax.imshow(residual, interpolation = 'nearest')
        ax.set_title('Image-Model')
        pl.colorbar(im)
        pl.show()

    if(np.isnan(maxima).sum() > 0):
        raise ValueError('get_ML_estimator - FATAL - NaNs found in maxima:', maxima)

    if(verbose):
        print 'Maxima found to be:', maxima

    ##Output Result
    if(outputHandle is not None):
        np.savetxt(outputHandle, np.array(maxima).reshape(1,maxima.shape[0]))
        
    ## Bias Correct
    if(biasCorrect == 0):
        Returned.append(maxima)
    elif(biasCorrect == 1):
        ana = mBias.analytic_GaussianLikelihood_Bias(maxima, fitParams, modelParams, order = biasCorrect, diffType = 'analytic')
        bc_maxima = maxima-ana

        ##Output Result
        if(bcoutputHandle is not None):
            np.savetxt(bcoutputHandle, np.array(bc_maxima).reshape(1,bc_maxima.shape[0]))

        if(verbose):
            print 'BC Maxima found to be:', bc_maxima

        ##Return minimised parameters
        Returned.append(maxima, bc_maxima)
    else:
        raise ValueError('get_ML_estimator - biasCorrect(ion) value entered is not applicable:'+ str(biasCorrect))


    ## Get Error on measurement. Brute error would have been constructed on the original brute force grid evaluation above.
    if(error.lower() == 'fisher'):
        err = fisher_Error_ML(maxima, fitParams, image, setParams, modelLookup) #Use finalised modelParams here?
        Returned.append(err)

    return Returned


def get_logLikelihood(parameters, pLabels, image, setParams, modelLookup = None, returnType = 'sum'):
    import math, sys
    import model_Production as modPro
    import surface_Brightness_Profiles as SBPro
    import generalManipulation
    """
    Returns the (-1.)*log-Likelihood as a Gaussian of lnL propto (I-Im)^2/sigma_n, where Im is image defined by dictionary ``modelParams'', and I is image being analysed, and sigma_n the pixel noise.
    Minimisiation routine should be directed to this function.

    Requires:
    parameters: flattened array of parameter values for free parameters (allows for external program to set variation in these params)
    pLabels: string tuple of length `parameters`, which is used to identify the parameters being varied. These labels should satisfy the modelParameter dictionary keys using in setting up the model.
    image: 2d <ndarray> of pixelised image.
    setParams: dictionary of fixed model parameters which sets the model SB profile being fit.
    modelLookup: An instance of the model lookup table, as set in model_Production module. If None, the the pixelised model image is re-evaluated for each change in parameters.
    returnType (default sum):
    ---`sum`: Total log-likelihood, summing over all pixels
    ---`pix`: log-likelihood evaluated per pixel. Returns ndarray of the same shape as the input image

    Returns:
    lnL <scalar>: -1*log_likelihood evaulated at entered model parameters

    """

    ##Set up dictionary based on model parameters. Shallow copy so changes do not overwrite the original
    modelParams = deepcopy(setParams)
    
    if((setParams['stamp_size']-np.array(image.shape)).sum() > 0):
        raise RuntimeError('get_logLikelihood - stamp size passed does not match image:', str(setParams['stamp_size']), ':', str( image.shape))
    
    parameters = generalManipulation.makeIterableList(parameters); pLabels = generalManipulation.makeIterableList(pLabels)
    if(len(parameters) != len(pLabels)):
        raise ValueError('get_logLikelihood - parameters and labels entered do not have the same length (iterable test): parameters:', str(parameters), ' labels:', str(pLabels))
    
    
    ##Vary parameters which are being varied as input
    modPro.set_modelParameter(modelParams, pLabels, parameters)
    
    ''' Deprecated for above
    for l in range(len(pLabels)):
    if(pLabels[l] not in modelParams):
    raw_input('Error setting model parameters in get_logLikelihood: Parameter not recognised. <Enter> to continue')
    else:
    modelParams[pLabels[l]] = parameters[l]
    '''
    
    #Test reasonable model values - Effectively applying a hard prior
    if(math.sqrt(modelParams['SB']['e1']**2. + modelParams['SB']['e2']**2.) >= 0.99):
        ##Set log-probability to be as small as possible
        return sys.float_info.max/10 #factor of 10 to avoid any chance of memory issues here
        #raise ValueError('get_logLikelihood - Invalid Ellipticty values set')
    if(modelParams['SB']['size'] <= 0.):
        return sys.float_info.max/10
    
    ''' Get Model'''
    if(modelLookup is not None and modelLookup['useLookup']):
        model = np.array(modPro.return_Model_Lookup(modelLookup, parameters)[0]) #First element of this routine is the model image itself
    else:
        model, disc = modPro.user_get_Pixelised_Model(modelParams, sbProfileFunc = SBPro.gaussian_SBProfile_Weave)

    ''' Model, lookup comparison '''
    '''
    modelEx, disc = modPro.user_get_Pixelised_Model(modelParams, sbProfileFunc = modPro.gaussian_SBProfile)
    print 'Model, lookup Comparison:', (model-modelEx).sum(), parameters
    import pylab as pl
    f = pl.figure()
    ax = f.add_subplot(211)
    im = ax.imshow(modelEx-model); ax.set_title('model - lookup'); pl.colorbar(im)
    ax = f.add_subplot(212)
    im = ax.imshow(modelEx/model); ax.set_title('model/lookup'); pl.colorbar(im)
    pl.show()
    '''

    """ DEPRECATED for multiple models
    if(model.shape != image.shape):
        print "\n\n Model shape: ", model.shape, " :: Image Shape:", image.shape 
        raise ValueError('get_logLikelihood - model returned is not of the same shape as the input image.')
    """
    
    #Flatten model
    model = model.flatten()

    #Construct log-Likelihood assuming Gaussian noise. As this will be minimised, remove the -1 preceeding
    if(vverbose):
        print 'Noise in ln-Like evaluation:', modelParams['noise']

    keepPix = returnType.lower() == 'pix' or returnType.lower() == 'all'

    pixlnL = np.array([])
    lnL = 0
    if(len(image.shape) == len(model.shape)+1):
        print "Considering sum over images", pLabels, parameters
        for i in range(image.shape[0]):
            tpixlnL = np.power(image[i]-model,2.)
            lnL += tpixlnL.sum()
            if(keepPix):
                pixlnL = np.append(pixlnL, tpixlnL)
    
    else:
        tpixlnL = np.power(image-model,2.)
        lnL += tpixlnL.sum()
        if(keepPix):
            pixlnL = np.append(pixlnL,tpixlnL)

    pixlnL *= 0.5/(modelParams['noise']**2.); lnL *= 0.5/(modelParams['noise']**2.)

    if(vverbose):
        print 'lnL:', lnL, [ str(pLabels[i])+':'+str(parameters[i]) for i in range(len(pLabels))]

    ##Model is noise free, so the noise must be seperately measured and passed in
    ## Answer is independent of noise provided invariant across image
    #lnL2 = 0.5*( (np.power(image-model,2.)).sum()/(modelParams['noise']**2.))
    if(returnType.lower() == 'sum'):
        return lnL
    elif(returnType.lower() == 'pix'):
        return pixlnL
    elif(returnType.lower() == 'all'):
        return [lnL, pixlnL]

###---------------- Derivatives of log-Likelihood -----------------------------------------------###

def differentiate_logLikelihood_Gaussian_Analytic(parameters, pLabels, image, setParams, modelLookup = None, returnType = None, order = 1, signModifier = -1.):
    import generalManipulation
    import model_Production as modPro
    from surface_Brightness_Profiles import gaussian_SBProfile_Weave
    '''
    Returns the analytic derivative of the Gaussian log-Likelihood (ignoring parameter-independent prefactor whose derivative is zero) for parameters labelled by pLabels.
    Uses analytic derivative of the pixelised model as given in differentiate_Pixelised_Model_Analytic routine of model_Production routine.

    *** Note: `noise` as defined in set params must the noise_std, and must accurately describe the noise properties of the image. ***

    Requires:
    parameters: flattened array of parameter values to vary (allows for external program to set variation in these params)
    pLabels: tuple of length `parameters`, which is used to identify the parameters being varied. These labels should satisfy the modelParameter dictionary keys using in setting up the model
    image: 2d <ndarray> of pixelised image
    setParams: dictionary of fixed model parameters which sets the model SB profile being fit.
    modelLookup: An instance of the model lookup table, as set in model_Production module
    returnType: IGNORED, but included so that this method mimic the call fingerprint of the log-Likelihood evaluation routine if used as part of a pre-fab minimisation routine.
    order: sets the order to which derivatives are taken. If order == 1, the return is a tuple (ndarray) of length len(parameters), which contains the first derivatives of all parameters. If order == 2, the retrun is a two-dimensional ndarray, where each element i,j gives the sendon derivative wrt parameter i and parameter j. Order >= 3 or <= 0 are not supported.
    signModifier: default -1. Result is multiplied by abs(signModifier)/signModifier, to change the sing of the output. This is required as the lnL routine actually returns -lnL = chi^2 where a minimisation routine is used. Thus, where the minimisation uses first derivatives, the signModifier should be postive, whilst for other applications (such as the fisher error) on requires the derivative of lnL, and so sign modifier must be negative. The absolute value of signModifier is unimportant.

    Returns:
    [dlnL/dbeta], repeated for all beta in order <1D ndarray>: derivative of -1*log_likelihood evaulated at entered model parameters if order == 1
    [[dlnL/dbeta_i dbeta_j]], repeated for all beta in order <2D ndarray>: second derivative of -1*log_likelihood evaulated at entered model parameters if order == 1

    Possible Extensions:
    -- In calculating second order derivatives, a nested loop is used. This is likely to be slow, and as this is used in producing fisher errors (and thus done every run-time), then this cold be a bottle-neck on the measurement of the ML point where errors are used

    Tests:
    -- Fisher error agrees wel with simulated output for error.
    '''
    
    #if(len(image.shape) > 1):
    #    raise ValueError("differentiate_logLikelihood_Gaussian_Analytic: This routine has not been extended to multiple realisations yet")

    ##To be useful as part of a minimisation routine, the arguements passed to this function must be the same as those passed to the ln-Likelihood evalutaion also. This suggest possibly two routines: one, like the model differentiation itself should just return the various derivatives, and a wrapper routine which produces only the relevent derivatives required for mimimisation
    ## Third order is ignored for now, as this wold require an edit to the methdo of calculating model derivatives, and it is unlikely that a third order derivative would ever really be necessary (excpet in the case where an analytic derivative of the model is wanted for the calculation of the bias, where simulations over many images are used: usually, the known statistics of the Gaussian lileihood can be used to remove this necessity anyway).


    ### First derivative only are needed, so for now this will be coded only to deal with first derivatives.
    ### Therefore, n = 1, permute = false by default
    ### Note, that this code is unlikely to speed up any computation provided that the derivative is calculated using SymPY. Therefore this must be addressed.

    ### Set up model parameters as input
    ##Set up dictionary based on model parameters. Shallow copy so changes do not overwrite the original
    modelParams = deepcopy(setParams)

    #if(setParams['stamp_size'] != image.shape):
    #    raise RuntimeError('differentiate_logLikelihood_Gaussian_Analytic - stamp size passed does not match image:', str(setParams['stamp_size']), ':', str( image.shape))

    ##Check whether parameters input are iterable and assign to a tuple if not: this allows both `parameters' and `pLabels' to be passed as e.g. a float and string and the method to still be used as it
    parameters = generalManipulation.makeIterableList(parameters); pLabels = generalManipulation.makeIterableList(pLabels)
    if(len(parameters) != len(pLabels)):
        raise ValueError('get_logLikelihood - parameters and labels entered do not have the same length (iterable test)')

    ##Vary parameters which are being varied as input
    modPro.set_modelParameter(modelParams, pLabels, parameters)

    ''' Get Model'''
    if(modelLookup is not None and modelLookup['useLookup']):
        model = np.array(modPro.return_Model_Lookup(modelLookup, parameters)[0]) #First element of this routine is the model image itself
    else:
        model = modPro.user_get_Pixelised_Model(modelParams, sbProfileFunc = gaussian_SBProfile_Weave)[0]

    ''' Get model derivatives '''    
    modDer = modPro.differentiate_Pixelised_Model_Analytic(modelParams, parameters, pLabels, n = 1, permute = False)
    #modDer stores only the n'th derivative of all parameters entered, stored as an nP*nPix*nPix array.

    print "modDer shape:", modDer.shape()

    #Flatten modDer to mimic flattened image
    modDer = [modDer[i].flatten() for i in range(nP)]
    print "modDer shape:", modDer.shape()
    raw_input()

    if(order == 2):
        ##Calculate 2nd derivative also
        modDer2 = modPro.differentiate_Pixelised_Model_Analytic(modelParams, parameters, pLabels, n = 2, permute = True)
            #modDer2 stores the 2nd derivative of all parameters entered, stored as an nP*nP*nPix*nPix array.

    ##Construct the result to be returned. This is a scalar array, with length equal to nP, and where each element corresponds to the gradient in that parameter direction
    nP = len(parameters)
    delI = image - model

    if(order == 1):
        res = np.zeros(nP)
        
        ##Create tdI, which stores dI in the same shape as modDer by adding a first dimension
        tdelI = np.zeros(modDer.shape); tdelI[:] = delI.copy()
        ##Alternatively: tdelI = np.repeat(delI.reshape((1,)+delI.shape), modDer.shape[0], axis = 0)

        ##Set derivative as sum_pix(delI*derI)/sig^2 for all parameters entered
        ## ReturnTypes other than sum could be implemented by removing the sum pats of this relation, however the implementation of fprime in the minimisation routines requires the return to be a 1D array containing the gradient in each direction.
        res = (tdelI*modDer).sum(axis = -1).sum(axis = -1)
        res /= (signModifier/abs(signModifier))*modelParams['noise']*modelParams['noise']
    elif(order == 2):
        res = np.zeros((nP,nP))
        ##This could and should be sped-up using two single loops rather than a nested loop, or by defining delI and dIm*dIm in the same dimension as modDer2
        ## Alternate speed-up is to implement with Weave
        for i in range(nP):
            for j in range(nP):
                res[i,j] = (delI*modDer2[i,j] - modDer[i]*modDer[j]).sum(axis = -1).sum(axis = -1)

        res /= (signModifier/abs(signModifier))*modelParams['noise']*modelParams['noise']


    return res
    
